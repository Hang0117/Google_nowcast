
#!/usr/bin/env python3
"""Concurrent Google nowcast scraper with multi-threading.

Uses ThreadPoolExecutor to scrape multiple cities in parallel.
"""
import os
import sys
import subprocess
import platform
from datetime import datetime, timedelta, timezone
from pathlib import Path
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading


def _install_dependencies():
    """Auto-install system and Python dependencies on Linux."""
    system = platform.system()
    
    if system != "Linux":
        # Skip on non-Linux systems (Windows, macOS)
        try:
            import selenium
            import pandas
            from apscheduler.schedulers.background import BackgroundScheduler
        except ImportError:
            print("âš ï¸  Warning: Some Python packages not installed. Please install manually:")
            print("   pip install selenium webdriver-manager pandas apscheduler pytz")
        return
    
    # Linux: Install Chrome and Python dependencies
    print("ğŸ”§ Checking and installing dependencies on Linux...")
    
    # Check if Chrome is installed
    chrome_check = subprocess.run(
        ["which", "google-chrome"], 
        capture_output=True
    )
    
    if chrome_check.returncode != 0:
        print("ğŸ“¦ Installing Google Chrome...")
        try:
            subprocess.run(["sudo", "apt-get", "update"], check=True, capture_output=True)
            subprocess.run(
                ["sudo", "bash", "-c", 
                 "wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - && "
                 "echo 'deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main' > /etc/apt/sources.list.d/google-chrome.list"],
                check=True, capture_output=True
            )
            subprocess.run(["sudo", "apt-get", "update"], check=True, capture_output=True)
            subprocess.run(
                ["sudo", "apt-get", "install", "-y", "google-chrome-stable"],
                check=True, capture_output=True
            )
            print("âœ… Google Chrome installed")
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸  Could not install Chrome: {e}")
            print("   Please install manually: sudo apt-get install -y google-chrome-stable")
    else:
        print("âœ… Google Chrome already installed")
    
    # Install Python packages
    print("ğŸ“¦ Installing Python packages...")
    packages = ["selenium", "webdriver-manager", "pandas", "apscheduler", "pytz"]
    try:
        subprocess.run(
            ["pip", "install", "-q"] + packages,
            check=True
        )
        print("âœ… Python packages installed")
    except subprocess.CalledProcessError as e:
        print(f"âš ï¸  Could not install Python packages: {e}")
        raise


# Auto-install dependencies when imported
_install_dependencies()

# Now import the required packages
from apscheduler.schedulers.background import BackgroundScheduler
import pandas as pd


def _chrome_driver(headless: bool = True):
    from selenium import webdriver
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.chrome.service import Service

    options = webdriver.ChromeOptions()
    if headless:
        options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"]) 
    options.add_experimental_option("useAutomationExtension", False)
    mobile_emulation = {"deviceName": "Nexus 5"}
    options.add_experimental_option("mobileEmulation", mobile_emulation)
    options.add_argument(
        "user-agent=Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36"
    )

    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)


def _accept_consent(driver):
    try:
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        btn = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.ID, "L2AGLb")))
        btn.click()
        time.sleep(0.3)
        return
    except Exception:
        pass
    try:
        from selenium.webdriver.common.by import By
        candidates = driver.find_elements(By.XPATH, "//button//*[text()='Accept all']/..|//button//*[text()='I agree']/..")
        if candidates:
            candidates[0].click()
            time.sleep(0.3)
    except Exception:
        pass


def scrape_nowcast_svg(
    city: str = "Fairfax, California, United States",
    city_id: str = "",
    headless: bool = True,
    save_json: bool = True,
    output_dir: str | Path | None = None,
    first_scrape_date: str | None = None,
):
    """Scrape rect heights from the SVG whose viewBox includes 1440 and 48."""
    start_time = time.time()
    
    try:
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
    except Exception as e:
        print(f"ERR [{city}]: selenium not available:", e)
        return None

    base_dir = Path(output_dir) if output_dir else Path(__file__).parent

    out = {
        "city": city,
        "city_id": city_id,
        "scrape_time": datetime.now(timezone.utc).isoformat(),
        "type": None,
        "viewBox": None,
        "points": []
    }

    driver = _chrome_driver(headless=headless)
    try:
        # start crawling timer
        crawl_start = time.time()
        driver.get("https://www.google.com/ncr?hl=en&gl=us")
        _accept_consent(driver)

        from urllib.parse import quote_plus
        q = quote_plus(f"weather {city}")
        driver.get(f"https://www.google.com/search?q={q}&hl=en&gl=us")

        time.sleep(3)
        
        # Save HTML page
        try:
            html_content = driver.page_source
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            html_dir = base_dir / "GoogleNowcastHTML" / folder_date
            html_dir.mkdir(parents=True, exist_ok=True)
            html_filename = f"{city_id}_{folder_date}.html"
            html_path = html_dir / html_filename
            html_path.write_text(html_content, encoding="utf-8")
            print(f"[{city}] Saved HTML: {html_filename}")
        except Exception as e:
            print(f"[{city}] Warning: Could not save HTML: {e}")
        
        crawl_time = time.time() - crawl_start
        print(f"[{city_id}] çˆ¬å–è€—æ—¶ï¼ˆå«ä¿å­˜HTMLï¼‰: {crawl_time:.2f}ç§’")
        
        # start parsing timer
        parse_start = time.time()
        
        # Check for reCAPTCHA robot verification
        check_robot_js = """
        const pageText = document.body.innerText;
        const hasRobotCheck = pageText.includes("I'm not a robot") || pageText.includes("unusual traffic");
        return hasRobotCheck;
        """
        is_robot_check = driver.execute_script(check_robot_js)
        if is_robot_check:
            print("âš  reCAPTCHA verification detected: 'I'm not a robot'")
            out["type"] = "robot"
            if save_json:
                folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
                file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
                outdir = base_dir / "Crawled" / folder_date
                outdir.mkdir(parents=True, exist_ok=True)
                fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
                fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
                print("Saved:", fname)
            return out

        js = """
        const all = document.querySelectorAll('svg');
        const withRects = [];
        for (const svg of all) {
            const rects = svg.querySelectorAll('rect');
            if (rects.length) {
                withRects.push({svg, viewBox: svg.getAttribute('viewBox'), rectCount: rects.length});
            }
        }
        let target = null;
        for (const info of withRects) {
            const vb = (info.viewBox || "");
            if (vb.includes('1440') && vb.includes('48')) { target = info; break; }
        }
        if (!target) {
            return {found:false, sample: withRects.slice(0, 10)};
        }
        const rects = target.svg.querySelectorAll('rect');
        const rows = [];
        for (let i=0;i<rects.length;i++){
            const r = rects[i];
            rows.push({
                idx:i,
                height: r.getAttribute('height')||'',
                fill: r.getAttribute('fill')||'',
                x: r.getAttribute('x')||'',
                y: r.getAttribute('y')||'',
                width: r.getAttribute('width')||''
            });
        }
        return {found:true, viewBox: target.viewBox, rects: rows};
        """

        result = driver.execute_script(js)
        if not result or not result.get("found"):
            print(f"[{city}] No target SVG found. Trying fallback div...")
            fallback_js = """
            const div = document.querySelector('div[jsname="Kt2ahd"].XhUg9e');
            if (!div) return {found: false, reason: 'no_kt2ahd_div'};
            const div1 = div.querySelector('.SnOHQb.tNxQIb');
            const div2 = div.querySelector('.jz8NAf.ApHyTb');
            if (!div1 && !div2) return {found: false, reason: 'no_target_divs'};
            const data = {
                div1_text: div1 ? div1.textContent.trim() : null,
                div2_text: div2 ? div2.textContent.trim() : null
            };
            return {found: true, source: 'fallback_div', data: data};
            """
            
            fallback_result = driver.execute_script(fallback_js)
            if fallback_result and fallback_result.get("found"):
                print(f"[{city}] Fallback OK: found divs")
                out["fallback_data"] = fallback_result.get("data")
                out["source"] = "fallback_div"
                out["type"] = "nowcast"
                result = {"viewBox": None, "rects": []}
            else:
                print(f"[{city}] Fallback div not found. Trying hourly forecast...")
                hourly_js = """
                const container = document.querySelector('[jsname="s2gQvd"].EDblX.HG5ZQb');
                if (!container) return { found: false, reason: 'no_hourly_container' };
                const items = container.querySelectorAll('[role="listitem"][aria-label]');
                if (!items || items.length === 0) {
                    return { found: false, reason: 'no_hourly_items' };
                }
                const labels = [];
                for (let i = 0; i < Math.min(6, items.length); i++) {
                    const ariaLabel = items[i].getAttribute('aria-label');
                    if (ariaLabel) labels.push(ariaLabel);
                }
                return { found: labels.length > 0, count: labels.length, labels: labels };
                """
                
                hourly_result = driver.execute_script(hourly_js)
                if hourly_result and hourly_result.get("found"):
                    print(f"[{city}] Hourly forecast OK: {hourly_result.get('count', 0)} items")
                    out["hourly_data"] = hourly_result.get("labels", [])
                    out["source"] = "hourly_aria_label"
                    out["type"] = "hourly"
                    result = {"viewBox": None, "rects": []}
                else:
                    html = driver.page_source
                    dbg = base_dir / f"debug_nowcast_{city.split(',')[0].replace(' ', '_')}.html"
                    dbg.write_text(html, encoding="utf-8")
                    reason = hourly_result.get('reason') if hourly_result else 'unknown'
                    print(f"[{city}] No data found (reason: {reason}). Wrote {dbg.name}")
                    # Delete debug file after saving
                    try:
                        import time as time_module
                        time_module.sleep(0.5)  # Brief delay to ensure file is written
                        dbg.unlink()  # Delete the file
                        print(f"[{city}] Debug file deleted: {dbg.name}")
                    except Exception as del_err:
                        print(f"[{city}] Could not delete debug file: {del_err}")
                    out["message"] = "no nowcast data now."
                    if save_json:
                        folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
                        file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
                        outdir = base_dir / "Crawled" / folder_date
                        outdir.mkdir(parents=True, exist_ok=True)
                        fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
                        fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
                    return out

        out["viewBox"] = result.get("viewBox")
        if result.get("source"):
            out["source"] = result.get("source")
        if not out["type"]:
            out["type"] = "nowcast"
        rows = result.get("rects") or []
        start = datetime.fromisoformat(out["scrape_time"])
        # If minute is odd, subtract 1 minute to make it even
        if start.minute % 2 == 1:
            start = start - timedelta(minutes=1)
        for row in rows:
            t = start + timedelta(minutes=int(row.get("idx", 0)) * 2)
            out["points"].append({
                "minute_index": int(row.get("idx", 0)),
                "time": t.strftime("%Y-%m-%d %H:%M"),
                "height": row.get("height"),
                "fill": row.get("fill"),
                "x": row.get("x"),
                "y": row.get("y"),
                "width": row.get("width")
            })

        if save_json and out["points"]:
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
            outdir = base_dir / "Crawled" / folder_date
            outdir.mkdir(parents=True, exist_ok=True)
            fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
            fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"[{city}] Saved: {fname.name}")
        elif save_json and out.get("fallback_data"):
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
            outdir = base_dir / "Crawled" / folder_date
            outdir.mkdir(parents=True, exist_ok=True)
            fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
            fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"[{city}] Saved: {fname.name}")
        elif save_json and out.get("hourly_data"):
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
            outdir = base_dir / "Crawled" / folder_date
            outdir.mkdir(parents=True, exist_ok=True)
            fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
            fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"[{city}] Saved: {fname.name}")

        parse_time = time.time() - parse_start
        total_time = time.time() - start_time
        print(f"[{city_id}] â±ï¸ è§£æè€—æ—¶: {parse_time:.2f}ç§’")
        print(f"[{city_id}] â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        return out

    except Exception as e:
        total_time = time.time() - start_time
        print(f"ERR [{city}] (æ€»è€—æ—¶ {total_time:.2f}ç§’):", e)
        return None
    finally:
        try:
            driver.quit()
        except Exception:
            pass


# Thread-safe counter for progress tracking
class ProgressTracker:
    def __init__(self, total):
        self.total = total
        self.completed = 0
        self.lock = threading.Lock()
    
    def increment(self):
        with self.lock:
            self.completed += 1
            return self.completed


def scrape_city_wrapper(city, city_id, headless, output_root, tracker, first_scrape_date):
    """Wrapper function for concurrent scraping."""
    result = scrape_nowcast_svg(city, city_id=city_id, headless=headless, save_json=True, output_dir=output_root, first_scrape_date=first_scrape_date)
    completed = tracker.increment()
    
    if result and result.get("points"):
        print(f"[{completed}/{tracker.total}] âœ“ {city_id}: {len(result['points'])} points")
    elif result and result.get("hourly_data"):
        print(f"[{completed}/{tracker.total}] âœ“ {city_id}: {len(result['hourly_data'])} hourly items")
    elif result and result.get("fallback_data"):
        print(f"[{completed}/{tracker.total}] âœ“ {city_id}: fallback data")
    else:
        print(f"[{completed}/{tracker.total}] âœ— {city_id}: No data")
    
    return city, result


def scrape_single_city(city, city_id, base_dir):
    """å•ä¸ªåŸå¸‚çš„çˆ¬å–ä»»åŠ¡ï¼ˆç”¨äºå®šæ—¶è°ƒåº¦ï¼‰"""
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] å¼€å§‹çˆ¬å– {city_id}")
    result = scrape_nowcast_svg(
        city=city,
        city_id=city_id,
        headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
        save_json=True,
        output_dir=base_dir
    )
    if result:
        if result.get("points"):
            print(f"âœ“ {city_id} å®Œæˆ: {len(result['points'])} ä¸ªæ•°æ®ç‚¹")
        elif result.get("type") == "robot":
            print(f"âš ï¸ {city_id} reCAPTCHA")
        else:
            print(f"âœ“ {city_id} å®Œæˆ")
    else:
        print(f"âœ— {city_id} å¤±è´¥")
    return result


def generate_random_schedule(total_stations, duration_hours=12, avg_scrape_time=15):
    """ç”ŸæˆéšæœºåŒ–çš„è°ƒåº¦æ—¶é—´è¡¨ï¼ˆè€ƒè™‘å®é™…çˆ¬å–è€—æ—¶ï¼‰
    
    Args:
        total_stations: æ€»ç«™ç‚¹æ•°
        duration_hours: æŒç»­æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        avg_scrape_time: å¹³å‡æ¯ä¸ªç«™ç‚¹çš„çˆ¬å–æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤15ç§’
        
    Returns:
        list: æ¯ä¸ªç«™ç‚¹çš„è°ƒåº¦ç§’æ•°åˆ—è¡¨ï¼ˆç›¸å¯¹äºå¼€å§‹æ—¶é—´ï¼‰
    """
    import random
    
    total_seconds = duration_hours * 3600
    # æ€»æ‰§è¡Œæ—¶é—´ = æ‰€æœ‰ç«™ç‚¹çš„çˆ¬å–æ—¶é—´æ€»å’Œ
    total_scrape_time = total_stations * avg_scrape_time
    # å¯ç”¨äºé—´éš”çš„æ—¶é—´ = æ€»æ—¶é—´ - æ€»æ‰§è¡Œæ—¶é—´
    available_interval_time = total_seconds - total_scrape_time
    
    if available_interval_time < 0:
        print(f"âš ï¸ è­¦å‘Š: {duration_hours}å°æ—¶ä¸è¶³ä»¥å®Œæˆ{total_stations}ä¸ªç«™ç‚¹ï¼ˆéœ€è¦{total_scrape_time/3600:.1f}å°æ—¶ï¼‰")
        print(f"   å»ºè®®å¢åŠ æŒç»­æ—¶é—´æˆ–å‡å°‘ç«™ç‚¹æ•°")
        # ç´§å¯†è°ƒåº¦ï¼Œé—´éš”æœ€å°åŒ–
        avg_interval = 1
    else:
        # å¹³å‡é—´éš” = å¯ç”¨é—´éš”æ—¶é—´ / ç«™ç‚¹æ•°
        avg_interval = available_interval_time / total_stations
    
    print(f"ğŸ“Š è°ƒåº¦å‚æ•°:")
    print(f"   æ€»æ—¶é—´: {total_seconds}ç§’ ({duration_hours}å°æ—¶)")
    print(f"   é¢„è®¡çˆ¬å–æ—¶é—´: {total_scrape_time}ç§’ ({total_scrape_time/3600:.2f}å°æ—¶)")
    print(f"   å¯ç”¨é—´éš”æ—¶é—´: {available_interval_time}ç§’ ({available_interval_time/3600:.2f}å°æ—¶)")
    print(f"   å¹³å‡é—´éš”: {avg_interval:.1f}ç§’\n")
    
    # ä¸ºæ¯ä¸ªç«™ç‚¹åˆ†é…ä¸€ä¸ªæ—¶é—´æ®µï¼Œç„¶ååœ¨æ—¶é—´æ®µå†…éšæœºåŒ–
    schedule = []
    for i in range(total_stations):
        # è®¡ç®—è¯¥ç«™ç‚¹çš„å¯åŠ¨æ—¶é—´æ®µèŒƒå›´ï¼ˆä¸åŒ…æ‹¬æ‰§è¡Œæ—¶é—´ï¼‰
        segment_start = int(i * avg_interval)
        segment_end = int((i + 1) * avg_interval)
        
        # åœ¨æ—¶é—´æ®µå†…éšæœºé€‰æ‹©ä¸€ä¸ªå¯åŠ¨æ—¶é—´ç‚¹
        random_time = random.randint(segment_start, min(segment_end, int(available_interval_time)))
        schedule.append(random_time)
    
    # æ‰“ä¹±é¡ºåºä½¿å…¶æ›´éšæœº
    random.shuffle(schedule)
    
    return schedule


def scrape_all_cities_concurrent(base_dir, csv_file='nowcast_crawl_list_v3.csv', max_workers=5):
    """å¹¶å‘çˆ¬å–æ‰€æœ‰åŸå¸‚çš„æ°”è±¡æ•°æ®
    
    Args:
        base_dir: è¾“å‡ºç›®å½•çš„åŸºç¡€è·¯å¾„
        csv_file: CSV æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º 'nowcast_crawl_list_v3.csv'
        max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤5ä¸ª
    """
    import random
    
    df = pd.read_csv(csv_file)
    name_list = df['name'].tolist()
    id_list = df['id'].tolist()
    
    # éšæœºæ‰“ä¹±ç«™ç‚¹é¡ºåº
    combined = list(zip(name_list, id_list))
    random.shuffle(combined)
    name_list, id_list = zip(*combined)
    name_list = list(name_list)
    id_list = list(id_list)
    
    output_root = Path(base_dir)
    first_scrape_date = datetime.now(timezone.utc).strftime("%Y%m%d%H")
    
    print(f"\n{'='*60}")
    print(f"å¼€å§‹å¹¶å‘çˆ¬å–ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"è¾“å‡ºæ–‡ä»¶å¤¹: {first_scrape_date}")
    print(f"æ€»åŸå¸‚æ•°: {len(name_list)}, å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
    print(f"{'='*60}\n")
    
    tracker = ProgressTracker(len(name_list))
    results = {}
    
    # Use ThreadPoolExecutor for concurrent scraping
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_city = {
            executor.submit(scrape_city_wrapper, city, city_id, False, output_root, tracker, first_scrape_date): (city, city_id)
            for city, city_id in zip(name_list, id_list)
        }
        
        # Process completed tasks
        for future in as_completed(future_to_city):
            city, city_id = future_to_city[future]
            try:
                city_name, result = future.result()
                results[city_name] = result
            except Exception as e:
                print(f"âœ— Exception for {city_id}: {e}")
                results[city] = None
    
    print(f"\n{'='*60}")
    print(f"çˆ¬å–ä»»åŠ¡å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æˆåŠŸ: {sum(1 for r in results.values() if r)}/{len(name_list)}")
    print(f"{'='*60}\n")
    
    return results


if __name__ == "__main__":
    import pytz
    import random
    
    # å®šä¹‰é…ç½®å‚æ•°
    CSV_FILE = 'nowcast_crawl_list_v3.csv'
    BASE_DIR = Path(__file__).parent
    DURATION_HOURS = 12  # 12å°æ—¶å†…å®Œæˆæ‰€æœ‰ç«™ç‚¹
    AVG_SCRAPE_TIME = 15  # æ¯ä¸ªç«™ç‚¹å¹³å‡çˆ¬å–æ—¶é—´ï¼ˆç§’ï¼‰
    
    # è®¾ç½®è°ƒåº¦å™¨
    scheduler = BackgroundScheduler(timezone='UTC')
    
    def scheduled_crawl_task():
        """å®šæ—¶æ‰§è¡Œçš„çˆ¬å–ä»»åŠ¡"""
        # è¯»å–ç«™ç‚¹åˆ—è¡¨
        df = pd.read_csv(CSV_FILE)
        name_list = df['name'].tolist()
        id_list = df['id'].tolist()
        total_stations = len(name_list)
        
        print("\n" + "="*60)
        print(f"å®šæ—¶ä»»åŠ¡è§¦å‘ - {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}")
        print("="*60)
        print(f"æ€»ç«™ç‚¹æ•°: {total_stations}")
        print(f"è°ƒåº¦å‘¨æœŸ: {DURATION_HOURS} å°æ—¶")
        print(f"é¢„è®¡å•ç«™è€—æ—¶: {AVG_SCRAPE_TIME} ç§’")
        print("="*60 + "\n")
        
        # ç”Ÿæˆéšæœºè°ƒåº¦æ—¶é—´è¡¨
        schedule = generate_random_schedule(total_stations, DURATION_HOURS, AVG_SCRAPE_TIME)
        
        # è·å–å½“å‰æ—¶é—´ä½œä¸ºèµ·å§‹æ—¶é—´
        start_time = datetime.now(timezone.utc)
        
        # ä¸ºæ¯ä¸ªç«™ç‚¹æ·»åŠ è°ƒåº¦ä»»åŠ¡
        scheduled_count = 0
        for i, (city, city_id, delay_seconds) in enumerate(zip(name_list, id_list, schedule)):
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            run_time = start_time + timedelta(seconds=delay_seconds)
            
            # æ·»åŠ ä¸€æ¬¡æ€§ä»»åŠ¡
            scheduler.add_job(
                scrape_single_city,
                'date',
                run_date=run_time,
                args=[city, city_id, BASE_DIR],
                id=f'scrape_{city_id}_{int(start_time.timestamp())}_{i}'
            )
            scheduled_count += 1
            
            # æ¯100ä¸ªç«™ç‚¹è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                print(f"å·²è°ƒåº¦ {i + 1}/{total_stations} ä¸ªç«™ç‚¹...")
        
        print(f"\nâœ“ æˆåŠŸè°ƒåº¦ {scheduled_count} ä¸ªç«™ç‚¹")
        print(f"âœ“ å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')}")
        
        # è®¡ç®—å®é™…é¢„è®¡ç»“æŸæ—¶é—´ï¼ˆæœ€åä¸€ä¸ªä»»åŠ¡çš„å¯åŠ¨æ—¶é—´ + çˆ¬å–æ—¶é—´ï¼‰
        actual_end_time = start_time + timedelta(seconds=max(schedule) + AVG_SCRAPE_TIME)
        print(f"âœ“ é¢„è®¡ç»“æŸ: {actual_end_time.strftime('%Y-%m-%d %H:%M:%S UTC')}")
        
        actual_duration = (actual_end_time - start_time).total_seconds() / 3600
        print(f"âœ“ å®é™…æŒç»­æ—¶é—´: {actual_duration:.2f} å°æ—¶")
        print(f"âœ“ é¦–ä¸ªä»»åŠ¡å°†åœ¨ {min(schedule)} ç§’åæ‰§è¡Œ")
        print(f"âœ“ æœ€åä»»åŠ¡å°†åœ¨ {max(schedule)} ç§’åå¯åŠ¨\n")
    
    # æ·»åŠ å®šæ—¶ä»»åŠ¡ï¼šæ¯å¤© UTC 0ç‚¹å’Œ12ç‚¹è§¦å‘
    scheduler.add_job(scheduled_crawl_task, 'cron', hour='0,12', minute='0')
    
    print("="*60)
    print("å®šæ—¶çˆ¬è™«å·²å¯åŠ¨ï¼ˆéšæœºè°ƒåº¦æ¨¡å¼ï¼‰")
    print("="*60)
    print(f"âœ“ è¾“å‡ºç›®å½•: {BASE_DIR}")
    print(f"âœ“ CSV æ–‡ä»¶: {CSV_FILE}")
    print(f"âœ“ è§¦å‘æ—¶é—´: æ¯å¤© UTC 00:00 å’Œ 12:00")
    print(f"âœ“ å½“å‰ UTC æ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')}")
    print("âœ“ æŒ‰ Ctrl+C åœæ­¢ç¨‹åº\n")
    
    # å¯åŠ¨è°ƒåº¦å™¨
    scheduler.start()
    
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼ˆå¯é€‰ï¼Œç”¨äºæµ‹è¯•ï¼‰
    # scheduled_crawl_task()
    
    # æŒç»­è¿è¡Œ
    try:
        while True:
            time.sleep(60)
    except KeyboardInterrupt:
        print("\n\nç¨‹åºå·²åœæ­¢")
        scheduler.shutdown()

