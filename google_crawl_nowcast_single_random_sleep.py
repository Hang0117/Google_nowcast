#!/usr/bin/env python3
"""Concurrent Google nowcast scraper with multi-threading.

Uses ThreadPoolExecutor to scrape multiple cities in parallel.
"""
import os
import sys
import subprocess
import platform
from datetime import datetime, timedelta, timezone
from pathlib import Path
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import random


def _install_dependencies():
    """Auto-install system and Python dependencies on Linux."""
    system = platform.system()
    
    if system != "Linux":
        # Skip on non-Linux systems (Windows, macOS)
        try:
            import selenium
            import pandas
            from apscheduler.schedulers.background import BackgroundScheduler
        except ImportError:
            print("âš ï¸  Warning: Some Python packages not installed. Please install manually:")
            print("   pip install selenium webdriver-manager pandas apscheduler pytz")
        return
    
    # Linux: Install Chrome and Python dependencies
    print("ğŸ”§ Checking and installing dependencies on Linux...")
    
    # Check if Chrome is installed
    chrome_check = subprocess.run(
        ["which", "google-chrome"], 
        capture_output=True
    )
    
    if chrome_check.returncode != 0:
        print("ğŸ“¦ Installing Google Chrome...")
        try:
            subprocess.run(["sudo", "apt-get", "update"], check=True, capture_output=True)
            subprocess.run(
                ["sudo", "bash", "-c", 
                 "wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - && "
                 "echo 'deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main' > /etc/apt/sources.list.d/google-chrome.list"],
                check=True, capture_output=True
            )
            subprocess.run(["sudo", "apt-get", "update"], check=True, capture_output=True)
            subprocess.run(
                ["sudo", "apt-get", "install", "-y", "google-chrome-stable"],
                check=True, capture_output=True
            )
            print("âœ… Google Chrome installed")
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸  Could not install Chrome: {e}")
            print("   Please install manually: sudo apt-get install -y google-chrome-stable")
    else:
        print("âœ… Google Chrome already installed")
    
    # Install Python packages
    print("ğŸ“¦ Installing Python packages...")
    packages = ["selenium", "webdriver-manager", "pandas", "apscheduler", "pytz"]
    try:
        subprocess.run(
            ["pip", "install", "-q"] + packages,
            check=True
        )
        print("âœ… Python packages installed")
    except subprocess.CalledProcessError as e:
        print(f"âš ï¸  Could not install Python packages: {e}")
        raise


# Auto-install dependencies when imported
_install_dependencies()

# Now import the required packages
from apscheduler.schedulers.background import BackgroundScheduler
import pandas as pd


def _chrome_driver(headless: bool = True):
    from selenium import webdriver
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.chrome.service import Service

    options = webdriver.ChromeOptions()
    if headless:
        options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"]) 
    options.add_experimental_option("useAutomationExtension", False)
    mobile_emulation = {"deviceName": "Nexus 5"}
    options.add_experimental_option("mobileEmulation", mobile_emulation)
    options.add_argument(
        "user-agent=Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36"
    )

    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)


def _accept_consent(driver):
    try:
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        btn = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.ID, "L2AGLb")))
        btn.click()
        time.sleep(0.3)
        return
    except Exception:
        pass
    try:
        from selenium.webdriver.common.by import By
        candidates = driver.find_elements(By.XPATH, "//button//*[text()='Accept all']/..|//button//*[text()='I agree']/..")
        if candidates:
            candidates[0].click()
            time.sleep(0.3)
    except Exception:
        pass


def scrape_nowcast_svg(
    city: str = "Fairfax, California, United States",
    city_id: str = "",
    headless: bool = True,
    save_json: bool = True,
    output_dir: str | Path | None = None,
    first_scrape_date: str | None = None,
):
    """Scrape rect heights from the SVG whose viewBox includes 1440 and 48."""
    try:
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
    except Exception as e:
        print(f"ERR [{city}]: selenium not available:", e)
        return None

    base_dir = Path(output_dir) if output_dir else Path(__file__).parent

    out = {
        "city": city,
        "city_id": city_id,
        "scrape_time": datetime.now(timezone.utc).isoformat(),
        "type": None,
        "viewBox": None,
        "points": []
    }

    driver = _chrome_driver(headless=headless)
    try:
        driver.get("https://www.google.com/ncr?hl=en&gl=us")
        _accept_consent(driver)

        from urllib.parse import quote_plus
        q = quote_plus(f"weather {city}")
        driver.get(f"https://www.google.com/search?q={q}&hl=en&gl=us")

        time.sleep(3)
        
        # Save HTML page
        try:
            html_content = driver.page_source
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            html_dir = base_dir / "GoogleNowcastHTML" / folder_date
            html_dir.mkdir(parents=True, exist_ok=True)
            html_filename = f"{city_id}_{folder_date}.html"
            html_path = html_dir / html_filename
            html_path.write_text(html_content, encoding="utf-8")
            print(f"[{city}] Saved HTML: {html_filename}")
        except Exception as e:
            print(f"[{city}] Warning: Could not save HTML: {e}")
        
        # Check for reCAPTCHA robot verification
        check_robot_js = """
        const pageText = document.body.innerText;
        const hasRobotCheck = pageText.includes("I'm not a robot") || pageText.includes("unusual traffic");
        return hasRobotCheck;
        """
        is_robot_check = driver.execute_script(check_robot_js)
        if is_robot_check:
            print("âš  reCAPTCHA verification detected: 'I'm not a robot'")
            out["type"] = "robot"
            if save_json:
                folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
                file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
                outdir = base_dir / "Crawled" / folder_date
                outdir.mkdir(parents=True, exist_ok=True)
                fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
                fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
                print("Saved:", fname)
            return out

        js = """
        const all = document.querySelectorAll('svg');
        const withRects = [];
        for (const svg of all) {
            const rects = svg.querySelectorAll('rect');
            if (rects.length) {
                withRects.push({svg, viewBox: svg.getAttribute('viewBox'), rectCount: rects.length});
            }
        }
        let target = null;
        for (const info of withRects) {
            const vb = (info.viewBox || "");
            if (vb.includes('1440') && vb.includes('48')) { target = info; break; }
        }
        if (!target) {
            return {found:false, sample: withRects.slice(0, 10)};
        }
        const rects = target.svg.querySelectorAll('rect');
        const rows = [];
        for (let i=0;i<rects.length;i++){
            const r = rects[i];
            rows.push({
                idx:i,
                height: r.getAttribute('height')||'',
                fill: r.getAttribute('fill')||'',
                x: r.getAttribute('x')||'',
                y: r.getAttribute('y')||'',
                width: r.getAttribute('width')||''
            });
        }
        return {found:true, viewBox: target.viewBox, rects: rows};
        """

        result = driver.execute_script(js)
        if not result or not result.get("found"):
            print(f"[{city}] No target SVG found. Trying fallback div...")
            fallback_js = """
            const div = document.querySelector('div[jsname="Kt2ahd"].XhUg9e');
            if (!div) return {found: false, reason: 'no_kt2ahd_div'};
            const div1 = div.querySelector('.SnOHQb.tNxQIb');
            const div2 = div.querySelector('.jz8NAf.ApHyTb');
            if (!div1 && !div2) return {found: false, reason: 'no_target_divs'};
            const data = {
                div1_text: div1 ? div1.textContent.trim() : null,
                div2_text: div2 ? div2.textContent.trim() : null
            };
            return {found: true, source: 'fallback_div', data: data};
            """
            
            fallback_result = driver.execute_script(fallback_js)
            if fallback_result and fallback_result.get("found"):
                print(f"[{city}] Fallback OK: found divs")
                out["fallback_data"] = fallback_result.get("data")
                out["source"] = "fallback_div"
                out["type"] = "nowcast"
                result = {"viewBox": None, "rects": []}
            else:
                print(f"[{city}] Fallback div not found. Trying hourly forecast...")
                hourly_js = """
                const container = document.querySelector('[jsname="s2gQvd"].EDblX.HG5ZQb');
                if (!container) return { found: false, reason: 'no_hourly_container' };
                const items = container.querySelectorAll('[role="listitem"][aria-label]');
                if (!items || items.length === 0) {
                    return { found: false, reason: 'no_hourly_items' };
                }
                const labels = [];
                for (let i = 0; i < Math.min(6, items.length); i++) {
                    const ariaLabel = items[i].getAttribute('aria-label');
                    if (ariaLabel) labels.push(ariaLabel);
                }
                return { found: labels.length > 0, count: labels.length, labels: labels };
                """
                
                hourly_result = driver.execute_script(hourly_js)
                if hourly_result and hourly_result.get("found"):
                    print(f"[{city}] Hourly forecast OK: {hourly_result.get('count', 0)} items")
                    out["hourly_data"] = hourly_result.get("labels", [])
                    out["source"] = "hourly_aria_label"
                    out["type"] = "hourly"
                    result = {"viewBox": None, "rects": []}
                else:
                    html = driver.page_source
                    dbg = base_dir / f"debug_nowcast_{city.split(',')[0].replace(' ', '_')}.html"
                    dbg.write_text(html, encoding="utf-8")
                    reason = hourly_result.get('reason') if hourly_result else 'unknown'
                    print(f"[{city}] No data found (reason: {reason}). Wrote {dbg.name}")
                    # Delete debug file after saving
                    try:
                        import time as time_module
                        time_module.sleep(0.5)  # Brief delay to ensure file is written
                        dbg.unlink()  # Delete the file
                        print(f"[{city}] Debug file deleted: {dbg.name}")
                    except Exception as del_err:
                        print(f"[{city}] Could not delete debug file: {del_err}")
                    out["message"] = "no nowcast data now."
                    if save_json:
                        folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
                        file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
                        outdir = base_dir / "Crawled" / folder_date
                        outdir.mkdir(parents=True, exist_ok=True)
                        fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
                        fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
                    return out

        out["viewBox"] = result.get("viewBox")
        if result.get("source"):
            out["source"] = result.get("source")
        if not out["type"]:
            out["type"] = "nowcast"
        rows = result.get("rects") or []
        start = datetime.fromisoformat(out["scrape_time"])
        # If minute is odd, subtract 1 minute to make it even
        if start.minute % 2 == 1:
            start = start - timedelta(minutes=1)
        for row in rows:
            t = start + timedelta(minutes=int(row.get("idx", 0)) * 2)
            out["points"].append({
                "minute_index": int(row.get("idx", 0)),
                "time": t.strftime("%Y-%m-%d %H:%M"),
                "height": row.get("height"),
                "fill": row.get("fill"),
                "x": row.get("x"),
                "y": row.get("y"),
                "width": row.get("width")
            })

        if save_json and out["points"]:
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
            outdir = base_dir / "Crawled" / folder_date
            outdir.mkdir(parents=True, exist_ok=True)
            fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
            fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"[{city}] Saved: {fname.name}")
        elif save_json and out.get("fallback_data"):
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
            outdir = base_dir / "Crawled" / folder_date
            outdir.mkdir(parents=True, exist_ok=True)
            fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
            fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"[{city}] Saved: {fname.name}")
        elif save_json and out.get("hourly_data"):
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
            outdir = base_dir / "Crawled" / folder_date
            outdir.mkdir(parents=True, exist_ok=True)
            fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
            fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"[{city}] Saved: {fname.name}")

        return out

    except Exception as e:
        print(f"ERR [{city}]:", e)
        return None
    finally:
        try:
            driver.quit()
        except Exception:
            pass


# Thread-safe counter for progress tracking
class ProgressTracker:
    def __init__(self, total):
        self.total = total
        self.completed = 0
        self.lock = threading.Lock()
    
    def increment(self):
        with self.lock:
            self.completed += 1
            return self.completed


def scrape_city_wrapper(city, city_id, headless, output_root, tracker, first_scrape_date):
    """Wrapper function for concurrent scraping."""
    result = scrape_nowcast_svg(city, city_id=city_id, headless=headless, save_json=True, output_dir=output_root, first_scrape_date=first_scrape_date)
    completed = tracker.increment()
    
    if result and result.get("points"):
        print(f"[{completed}/{tracker.total}] âœ“ {city_id}: {len(result['points'])} points")
    elif result and result.get("hourly_data"):
        print(f"[{completed}/{tracker.total}] âœ“ {city_id}: {len(result['hourly_data'])} hourly items")
    elif result and result.get("fallback_data"):
        print(f"[{completed}/{tracker.total}] âœ“ {city_id}: fallback data")
    else:
        print(f"[{completed}/{tracker.total}] âœ— {city_id}: No data")
    
    return city, result


def scrape_all_cities_concurrent(base_dir, csv_file='nowcast_crawl_list_v3.csv', max_workers=5, total_duration_hours=12, avg_scrape_time=15):
    """å¹¶å‘çˆ¬å–æ‰€æœ‰åŸå¸‚çš„æ°”è±¡æ•°æ®ï¼Œåœ¨æŒ‡å®šæ—¶é—´å†…åˆ†æ•£æ‰§è¡Œ
    
    Args:
        base_dir: è¾“å‡ºç›®å½•çš„åŸºç¡€è·¯å¾„
        csv_file: CSV æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º 'nowcast_crawl_list_v3.csv'
        max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤5ä¸ª
        total_duration_hours: æ€»æ‰§è¡Œæ—¶é•¿ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤12å°æ—¶
        avg_scrape_time: æ¯ä¸ªç«™ç‚¹å¹³å‡çˆ¬å–æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤15ç§’
    """
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    output_root = Path(base_dir)
    crawled_dir = output_root / "Crawled"
    html_dir = output_root / "GoogleNowcastHTML"
    
    if not crawled_dir.exists():
        crawled_dir.mkdir(parents=True, exist_ok=True)
        print(f"âœ“ åˆ›å»ºç›®å½•: {crawled_dir}")
    
    if not html_dir.exists():
        html_dir.mkdir(parents=True, exist_ok=True)
        print(f"âœ“ åˆ›å»ºç›®å½•: {html_dir}")
    
    df = pd.read_csv(csv_file)
    
    # randomly shuffle the DataFrame
    df = df.sample(frac=1, random_state=None).reset_index(drop=True)
    
    name_list = df['name'].tolist()
    id_list = df['id'].tolist()
    total_cities = len(name_list)
    
    # calculate total available time in seconds
    total_seconds = total_duration_hours * 3600
    total_scrape_time = total_cities * avg_scrape_time
    total_interval_time = total_seconds - total_scrape_time
    
    if total_interval_time < 0:
        print(f"âš ï¸  è­¦å‘Š: {total_cities} ä¸ªç«™ç‚¹éœ€è¦çº¦ {total_scrape_time/3600:.2f} å°æ—¶ï¼Œè¶…è¿‡è®¾å®šçš„ {total_duration_hours} å°æ—¶")
        total_interval_time = 0
    
    # calculate average interval time per city
    avg_interval = total_interval_time / total_cities if total_cities > 0 else 0
    
    # generate random intervals for each city (between 50% and 150% of the average)
    intervals = []
    for i in range(total_cities):
        if avg_interval > 0:
            # Random offset Â±50%
            random_interval = avg_interval * random.uniform(0.5, 1.5)
            intervals.append(random_interval)
        else:
            intervals.append(0)
    
    # Adjust intervals to ensure total duration is close to target
    if sum(intervals) > 0:
        scale_factor = total_interval_time / sum(intervals)
        intervals = [interval * scale_factor for interval in intervals]
    
    first_scrape_date = datetime.now(timezone.utc).strftime("%Y%m%d%H")
    
    print(f"\n{'='*60}")
    print(f"å¼€å§‹åˆ†æ•£çˆ¬å–ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"è¾“å‡ºæ–‡ä»¶å¤¹: {first_scrape_date}")
    print(f"æ€»åŸå¸‚æ•°: {total_cities}, å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
    print(f"é¢„è®¡æ€»æ—¶é•¿: {total_duration_hours} å°æ—¶ ({total_seconds/3600:.2f}h)")
    print(f"é¢„è®¡çˆ¬å–æ—¶é—´: {total_scrape_time/3600:.2f} å°æ—¶")
    print(f"é¢„è®¡é—´éš”æ—¶é—´: {total_interval_time/3600:.2f} å°æ—¶")
    print(f"å¹³å‡ç«™ç‚¹é—´éš”: {avg_interval:.1f} ç§’ (éšæœºåç§» Â±50%)")
    print(f"âœ“ åŸå¸‚åˆ—è¡¨å·²éšæœºæ‰“ä¹±")
    print(f"{'='*60}\n")
    
    tracker = ProgressTracker(total_cities)
    results = {}
    start_time = time.time()
    
    # Use ThreadPoolExecutor for concurrent scraping
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # é€ä¸ªæäº¤ä»»åŠ¡ï¼Œç­‰å¾…å®Œæˆåå†ç­‰å¾…éšæœºé—´éš”
        for idx, (city, city_id) in enumerate(zip(name_list, id_list)):
            # æäº¤ä»»åŠ¡å¹¶ç«‹å³ç­‰å¾…å®Œæˆ
            future = executor.submit(scrape_city_wrapper, city, city_id, False, output_root, tracker, first_scrape_date)
            
            try:
                city_name, result = future.result()  # ç­‰å¾…ä»»åŠ¡å®Œæˆï¼ˆæµè§ˆå™¨å…³é—­ï¼‰
                results[city_name] = result
            except Exception as e:
                print(f"âœ— Exception for {city_id}: {e}")
                results[city] = None
            
            # ä»»åŠ¡å®Œæˆåï¼Œåœ¨æäº¤ä¸‹ä¸€ä¸ªä»»åŠ¡å‰ç­‰å¾…éšæœºé—´éš”ï¼ˆæœ€åä¸€ä¸ªä»»åŠ¡ä¸éœ€è¦ç­‰å¾…ï¼‰
            if idx < total_cities - 1:
                sleep_time = intervals[idx]
                if sleep_time > 0:
                    elapsed = time.time() - start_time
                    expected_elapsed = sum(intervals[:idx+1]) + (idx + 1) * avg_scrape_time
                    # è°ƒæ•´sleepæ—¶é—´ä»¥ä¿æŒæ•´ä½“èŠ‚å¥
                    adjusted_sleep = max(0, sleep_time - max(0, elapsed - expected_elapsed))
                    if adjusted_sleep > 0:
                        print(f"â³ ç­‰å¾… {adjusted_sleep:.1f} ç§’åç»§ç»­...")
                        time.sleep(adjusted_sleep)
    
    elapsed_time = time.time() - start_time
    print(f"\n{'='*60}")
    print(f"çˆ¬å–ä»»åŠ¡å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"å®é™…ç”¨æ—¶: {elapsed_time/3600:.2f} å°æ—¶ ({elapsed_time:.0f} ç§’)")
    print(f"æˆåŠŸ: {sum(1 for r in results.values() if r)}/{total_cities}")
    print(f"{'='*60}\n")
    
    return results


if __name__ == "__main__":
    import pytz
    
    # settings parameters
    CSV_FILE = 'nowcast_crawl_list_v3.csv'
    MAX_WORKERS = 1
    BASE_DIR = Path(__file__).parent
    TOTAL_DURATION_HOURS = 12  # æ¯æ¬¡çˆ¬å–ä»»åŠ¡åœ¨12å°æ—¶å†…å®Œæˆ
    AVG_SCRAPE_TIME = 15  # æ¯ä¸ªç«™ç‚¹å¹³å‡çˆ¬å–æ—¶é—´ï¼ˆç§’ï¼‰
    
    # è®¾ç½®åŒ—äº¬æ—¶åŒº
    beijing_tz = pytz.timezone('Asia/Shanghai')
    
    # ä½¿ç”¨ APScheduler é…ç½® UTC æ—¶åŒºçš„å®šæ—¶ä»»åŠ¡
    scheduler = BackgroundScheduler(timezone='UTC')
    
    # æ¯å¤© UTC æ—¶é—´ 0ç‚¹ã€6ç‚¹ã€12ç‚¹ã€18ç‚¹å„æ‰§è¡Œä¸€æ¬¡
    # scheduler.add_job(lambda: scrape_all_cities_concurrent(base_dir=BASE_DIR, csv_file=CSV_FILE, max_workers=MAX_WORKERS, total_duration_hours=TOTAL_DURATION_HOURS, avg_scrape_time=AVG_SCRAPE_TIME), 'cron', hour='0,6,12,18')
    scheduler.add_job(lambda: scrape_all_cities_concurrent(base_dir=BASE_DIR, csv_file=CSV_FILE, max_workers=MAX_WORKERS, total_duration_hours=TOTAL_DURATION_HOURS, avg_scrape_time=AVG_SCRAPE_TIME), 'cron', hour='0')
    scheduler.start()
    
    print("âœ“ å®šæ—¶çˆ¬è™«å·²å¯åŠ¨ï¼ˆåˆ†æ•£çˆ¬å–æ¨¡å¼ï¼‰")
    print(f"âœ“ è¾“å‡ºç›®å½•: {BASE_DIR}")
    print(f"âœ“ CSV æ–‡ä»¶: {CSV_FILE}")
    print(f"âœ“ å°†åœ¨æ¯å¤© UTC æ—¶é—´ 00:00 æ‰§è¡Œçˆ¬å–ä»»åŠ¡")
    print(f"âœ“ å¹¶å‘çº¿ç¨‹æ•°: {MAX_WORKERS}")
    print(f"âœ“ æ¯æ¬¡ä»»åŠ¡æ—¶é•¿: {TOTAL_DURATION_HOURS} å°æ—¶")
    print(f"âœ“ å¹³å‡çˆ¬å–æ—¶é—´: {AVG_SCRAPE_TIME} ç§’/ç«™ç‚¹")
    print(f"âœ“ å½“å‰åŒ—äº¬æ—¶é—´: {datetime.now(beijing_tz).strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"âœ“ å½“å‰ UTC æ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')}")
    print("âœ“ æŒ‰ Ctrl+C åœæ­¢ç¨‹åº\n")
    
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼ˆå¯é€‰ï¼‰
    scrape_all_cities_concurrent(base_dir=BASE_DIR, csv_file=CSV_FILE, max_workers=MAX_WORKERS, total_duration_hours=TOTAL_DURATION_HOURS, avg_scrape_time=AVG_SCRAPE_TIME)
    
    # æŒç»­è¿è¡Œè°ƒåº¦å™¨
    try:
        while True:
            time.sleep(60)
    except KeyboardInterrupt:
        print("\n\nç¨‹åºå·²åœæ­¢")
        scheduler.shutdown()
