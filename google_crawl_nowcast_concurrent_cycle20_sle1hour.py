#!/usr/bin/env python3
"""Concurrent Google nowcast scraper with 20min work / 1hour rest cycle.

Uses ThreadPoolExecutor to scrape multiple cities in parallel.
Works in cycles: 20 minutes scraping, then 1 hour rest, until all cities done.
"""
import os
import sys
import subprocess
import platform
from datetime import datetime, timedelta, timezone
from pathlib import Path
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading


def _install_dependencies():
    """Auto-install system and Python dependencies on Linux."""
    system = platform.system()
    
    if system != "Linux":
        # Skip on non-Linux systems (Windows, macOS)
        try:
            import selenium
            import pandas
            from apscheduler.schedulers.background import BackgroundScheduler
        except ImportError:
            print("âš ï¸  Warning: Some Python packages not installed. Please install manually:")
            print("   pip install selenium webdriver-manager pandas apscheduler pytz")
        return
    
    # Linux: Install Chrome and Python dependencies
    print("ğŸ”§ Checking and installing dependencies on Linux...")
    
    # Check if Chrome is installed
    chrome_check = subprocess.run(
        ["which", "google-chrome"], 
        capture_output=True
    )
    
    if chrome_check.returncode != 0:
        print("ğŸ“¦ Installing Google Chrome...")
        try:
            subprocess.run(["sudo", "apt-get", "update"], check=True, capture_output=True)
            subprocess.run(
                ["sudo", "bash", "-c", 
                 "wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - && "
                 "echo 'deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main' > /etc/apt/sources.list.d/google-chrome.list"],
                check=True, capture_output=True
            )
            subprocess.run(["sudo", "apt-get", "update"], check=True, capture_output=True)
            subprocess.run(
                ["sudo", "apt-get", "install", "-y", "google-chrome-stable"],
                check=True, capture_output=True
            )
            print("âœ… Google Chrome installed")
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸  Could not install Chrome: {e}")
            print("   Please install manually: sudo apt-get install -y google-chrome-stable")
    else:
        print("âœ… Google Chrome already installed")
    
    # Install Python packages
    print("ğŸ“¦ Installing Python packages...")
    packages = ["selenium", "webdriver-manager", "pandas", "apscheduler", "pytz"]
    try:
        subprocess.run(
            ["pip", "install", "-q"] + packages,
            check=True
        )
        print("âœ… Python packages installed")
    except subprocess.CalledProcessError as e:
        print(f"âš ï¸  Could not install Python packages: {e}")
        raise


# Auto-install dependencies when imported
_install_dependencies()

# Now import the required packages
from apscheduler.schedulers.background import BackgroundScheduler
import pandas as pd


def _chrome_driver(headless: bool = True):
    from selenium import webdriver
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.chrome.service import Service

    options = webdriver.ChromeOptions()
    if headless:
        options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"]) 
    options.add_experimental_option("useAutomationExtension", False)
    mobile_emulation = {"deviceName": "Nexus 5"}
    options.add_experimental_option("mobileEmulation", mobile_emulation)
    options.add_argument(
        "user-agent=Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36"
    )

    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)


def _accept_consent(driver):
    try:
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        btn = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.ID, "L2AGLb")))
        btn.click()
        time.sleep(0.3)
        return
    except Exception:
        pass
    try:
        from selenium.webdriver.common.by import By
        candidates = driver.find_elements(By.XPATH, "//button//*[text()='Accept all']/..|//button//*[text()='I agree']/..")
        if candidates:
            candidates[0].click()
            time.sleep(0.3)
    except Exception:
        pass


def scrape_nowcast_svg(
    city: str = "Fairfax, California, United States",
    city_id: str = "",
    headless: bool = True,
    save_json: bool = True,
    output_dir: str | Path | None = None,
    first_scrape_date: str | None = None,
):
    """Scrape rect heights from the SVG whose viewBox includes 1440 and 48."""
    try:
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
    except Exception as e:
        print(f"ERR [{city}]: selenium not available:", e)
        return None

    base_dir = Path(output_dir) if output_dir else Path(__file__).parent

    out = {
        "city": city,
        "city_id": city_id,
        "scrape_time": datetime.now(timezone.utc).isoformat(),
        "type": None,
        "viewBox": None,
        "points": []
    }

    driver = _chrome_driver(headless=headless)
    try:
        driver.get("https://www.google.com/ncr?hl=en&gl=us")
        _accept_consent(driver)

        from urllib.parse import quote_plus
        q = quote_plus(f"weather {city}")
        driver.get(f"https://www.google.com/search?q={q}&hl=en&gl=us")

        time.sleep(3)
        
        # Save HTML page
        try:
            html_content = driver.page_source
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            html_dir = base_dir / "GoogleNowcastHTML" / folder_date
            html_dir.mkdir(parents=True, exist_ok=True)
            html_filename = f"{city_id}_{folder_date}.html"
            html_path = html_dir / html_filename
            html_path.write_text(html_content, encoding="utf-8")
            print(f"[{city}] Saved HTML: {html_filename}")
        except Exception as e:
            print(f"[{city}] Warning: Could not save HTML: {e}")
        
        # Check for reCAPTCHA robot verification
        check_robot_js = """
        const pageText = document.body.innerText;
        const hasRobotCheck = pageText.includes("I'm not a robot") || pageText.includes("unusual traffic");
        return hasRobotCheck;
        """
        is_robot_check = driver.execute_script(check_robot_js)
        if is_robot_check:
            print("âš  reCAPTCHA verification detected: 'I'm not a robot'")
            out["type"] = "robot"
            if save_json:
                folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
                file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
                outdir = base_dir / "Crawled" / folder_date
                outdir.mkdir(parents=True, exist_ok=True)
                fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
                fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
                print("Saved:", fname)
            return out

        js = """
        const all = document.querySelectorAll('svg');
        const withRects = [];
        for (const svg of all) {
            const rects = svg.querySelectorAll('rect');
            if (rects.length) {
                withRects.push({svg, viewBox: svg.getAttribute('viewBox'), rectCount: rects.length});
            }
        }
        let target = null;
        for (const info of withRects) {
            const vb = (info.viewBox || "");
            if (vb.includes('1440') && vb.includes('48')) { target = info; break; }
        }
        if (!target) {
            return {found:false, sample: withRects.slice(0, 10)};
        }
        const rects = target.svg.querySelectorAll('rect');
        const rows = [];
        for (let i=0;i<rects.length;i++){
            const r = rects[i];
            rows.push({
                idx:i,
                height: r.getAttribute('height')||'',
                fill: r.getAttribute('fill')||'',
                x: r.getAttribute('x')||'',
                y: r.getAttribute('y')||'',
                width: r.getAttribute('width')||''
            });
        }
        return {found:true, viewBox: target.viewBox, rects: rows};
        """

        result = driver.execute_script(js)
        if not result or not result.get("found"):
            print(f"[{city}] No target SVG found. Trying fallback div...")
            fallback_js = """
            const div = document.querySelector('div[jsname="Kt2ahd"].XhUg9e');
            if (!div) return {found: false, reason: 'no_kt2ahd_div'};
            const div1 = div.querySelector('.SnOHQb.tNxQIb');
            const div2 = div.querySelector('.jz8NAf.ApHyTb');
            if (!div1 && !div2) return {found: false, reason: 'no_target_divs'};
            const data = {
                div1_text: div1 ? div1.textContent.trim() : null,
                div2_text: div2 ? div2.textContent.trim() : null
            };
            return {found: true, source: 'fallback_div', data: data};
            """
            
            fallback_result = driver.execute_script(fallback_js)
            if fallback_result and fallback_result.get("found"):
                print(f"[{city}] Fallback OK: found divs")
                out["fallback_data"] = fallback_result.get("data")
                out["source"] = "fallback_div"
                out["type"] = "nowcast"
                result = {"viewBox": None, "rects": []}
            else:
                print(f"[{city}] Fallback div not found. Trying hourly forecast...")
                hourly_js = """
                const container = document.querySelector('[jsname="s2gQvd"].EDblX.HG5ZQb');
                if (!container) return { found: false, reason: 'no_hourly_container' };
                const items = container.querySelectorAll('[role="listitem"][aria-label]');
                if (!items || items.length === 0) {
                    return { found: false, reason: 'no_hourly_items' };
                }
                const labels = [];
                for (let i = 0; i < Math.min(6, items.length); i++) {
                    const ariaLabel = items[i].getAttribute('aria-label');
                    if (ariaLabel) labels.push(ariaLabel);
                }
                return { found: labels.length > 0, count: labels.length, labels: labels };
                """
                
                hourly_result = driver.execute_script(hourly_js)
                if hourly_result and hourly_result.get("found"):
                    print(f"[{city}] Hourly forecast OK: {hourly_result.get('count', 0)} items")
                    out["hourly_data"] = hourly_result.get("labels", [])
                    out["source"] = "hourly_aria_label"
                    out["type"] = "hourly"
                    result = {"viewBox": None, "rects": []}
                else:
                    html = driver.page_source
                    dbg = base_dir / f"debug_nowcast_{city.split(',')[0].replace(' ', '_')}.html"
                    dbg.write_text(html, encoding="utf-8")
                    reason = hourly_result.get('reason') if hourly_result else 'unknown'
                    print(f"[{city}] No data found (reason: {reason}). Wrote {dbg.name}")
                    # Delete debug file after saving
                    try:
                        import time as time_module
                        time_module.sleep(0.5)  # Brief delay to ensure file is written
                        dbg.unlink()  # Delete the file
                        print(f"[{city}] Debug file deleted: {dbg.name}")
                    except Exception as del_err:
                        print(f"[{city}] Could not delete debug file: {del_err}")
                    out["message"] = "no nowcast data now."
                    if save_json:
                        folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
                        file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
                        outdir = base_dir / "Crawled" / folder_date
                        outdir.mkdir(parents=True, exist_ok=True)
                        fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
                        fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
                    return out

        out["viewBox"] = result.get("viewBox")
        if result.get("source"):
            out["source"] = result.get("source")
        if not out["type"]:
            out["type"] = "nowcast"
        rows = result.get("rects") or []
        start = datetime.fromisoformat(out["scrape_time"])
        # If minute is odd, subtract 1 minute to make it even
        if start.minute % 2 == 1:
            start = start - timedelta(minutes=1)
        for row in rows:
            t = start + timedelta(minutes=int(row.get("idx", 0)) * 2)
            out["points"].append({
                "minute_index": int(row.get("idx", 0)),
                "time": t.strftime("%Y-%m-%d %H:%M"),
                "height": row.get("height"),
                "fill": row.get("fill"),
                "x": row.get("x"),
                "y": row.get("y"),
                "width": row.get("width")
            })

        if save_json and out["points"]:
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
            outdir = base_dir / "Crawled" / folder_date
            outdir.mkdir(parents=True, exist_ok=True)
            fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
            fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"[{city}] Saved: {fname.name}")
        elif save_json and out.get("fallback_data"):
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
            outdir = base_dir / "Crawled" / folder_date
            outdir.mkdir(parents=True, exist_ok=True)
            fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
            fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"[{city}] Saved: {fname.name}")
        elif save_json and out.get("hourly_data"):
            folder_date = first_scrape_date if first_scrape_date else datetime.now(timezone.utc).strftime("%Y%m%d%H")
            file_timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
            outdir = base_dir / "Crawled" / folder_date
            outdir.mkdir(parents=True, exist_ok=True)
            fname = outdir / f"nowcast_{city_id}_{file_timestamp}.json"
            fname.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"[{city}] Saved: {fname.name}")

        return out

    except Exception as e:
        print(f"ERR [{city}]:", e)
        return None
    finally:
        try:
            driver.quit()
        except Exception:
            pass


# Thread-safe counter for progress tracking
class ProgressTracker:
    def __init__(self, total):
        self.total = total
        self.completed = 0
        self.lock = threading.Lock()
    
    def increment(self):
        with self.lock:
            self.completed += 1
            return self.completed


def scrape_city_wrapper(city, city_id, headless, output_root, tracker, first_scrape_date):
    """Wrapper function for concurrent scraping."""
    result = scrape_nowcast_svg(city, city_id=city_id, headless=headless, save_json=True, output_dir=output_root, first_scrape_date=first_scrape_date)
    completed = tracker.increment()
    
    if result and result.get("points"):
        print(f"[{completed}/{tracker.total}] âœ“ {city_id}: {len(result['points'])} points")
    elif result and result.get("hourly_data"):
        print(f"[{completed}/{tracker.total}] âœ“ {city_id}: {len(result['hourly_data'])} hourly items")
    elif result and result.get("fallback_data"):
        print(f"[{completed}/{tracker.total}] âœ“ {city_id}: fallback data")
    else:
        print(f"[{completed}/{tracker.total}] âœ— {city_id}: No data")
    
    return city, result


def scrape_all_cities_concurrent(base_dir, csv_file='nowcast_crawl_list_v3.csv', max_workers=5, work_duration_minutes=20, rest_duration_minutes=60):
    """å¹¶å‘çˆ¬å–æ‰€æœ‰åŸå¸‚çš„æ°”è±¡æ•°æ®ï¼Œå·¥ä½œ/ä¼‘æ¯å¾ªç¯æ¨¡å¼
    
    Args:
        base_dir: è¾“å‡ºç›®å½•çš„åŸºç¡€è·¯å¾„
        csv_file: CSV æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º 'nowcast_crawl_list_v3.csv'
        max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤5ä¸ª
        work_duration_minutes: å·¥ä½œæ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤20åˆ†é’Ÿ
        rest_duration_minutes: ä¼‘æ¯æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤60åˆ†é’Ÿ
    """
    df = pd.read_csv(csv_file)
    name_list = df['name'].tolist()
    id_list = df['id'].tolist()
    output_root = Path(base_dir)
    first_scrape_date = datetime.now(timezone.utc).strftime("%Y%m%d%H")
    
    total = len(name_list)
    work_duration = timedelta(minutes=work_duration_minutes)
    rest_duration = timedelta(minutes=rest_duration_minutes)
    
    print(f"\n{'='*60}")
    print(f"å¼€å§‹å¹¶å‘çˆ¬å–ä»»åŠ¡ï¼ˆå¾ªç¯æ¨¡å¼ï¼‰- {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"è¾“å‡ºæ–‡ä»¶å¤¹: {first_scrape_date}")
    print(f"æ€»åŸå¸‚æ•°: {total}, å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
    print(f"å·¥ä½œæ—¶é•¿: {work_duration_minutes}åˆ†é’Ÿ, ä¼‘æ¯æ—¶é•¿: {rest_duration_minutes}åˆ†é’Ÿ")
    print(f"{'='*60}\n")
    
    tracker = ProgressTracker(total)
    results = {}
    pending_cities = list(zip(name_list, id_list))  # å¾…çˆ¬å–çš„åŸå¸‚é˜Ÿåˆ—
    cycle_num = 1
    
    while pending_cities:
        cycle_start = datetime.now(timezone.utc)
        window_end = cycle_start + work_duration
        
        print(f"\n{'='*60}")
        print(f"ã€ç¬¬ {cycle_num} è½®å·¥ä½œå‘¨æœŸå¼€å§‹ã€‘")
        print(f"å¼€å§‹æ—¶é—´: {cycle_start.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"é¢„è®¡ç»“æŸ: {window_end.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"å‰©ä½™åŸå¸‚: {len(pending_cities)}/{total}")
        print(f"{'='*60}\n")
        
        # åœ¨æœ¬è½®å·¥ä½œçª—å£å†…å¹¶å‘çˆ¬å–
        cycle_results = []
        executor = ThreadPoolExecutor(max_workers=max_workers)
        futures_dict = {}
        completed_in_cycle = []
        
        # åŠ¨æ€æäº¤ä»»åŠ¡ï¼Œåœ¨å·¥ä½œçª—å£å†…æ§åˆ¶æäº¤æ•°é‡
        city_index = 0
        
        # å…ˆæäº¤ç¬¬ä¸€æ‰¹ä»»åŠ¡ï¼ˆmax_workersä¸ªï¼‰
        while city_index < len(pending_cities) and city_index < max_workers:
            city, city_id = pending_cities[city_index]
            future = executor.submit(
                scrape_city_wrapper,
                city,
                city_id,
                False,
                output_root,
                tracker,
                first_scrape_date
            )
            futures_dict[future] = (city, city_id)
            city_index += 1
        
        # æ”¶é›†å®Œæˆçš„ä»»åŠ¡ï¼Œåœ¨å·¥ä½œçª—å£å†…ç»§ç»­æäº¤æ–°ä»»åŠ¡
        # ä½¿ç”¨whileå¾ªç¯åŠ¨æ€å¤„ç†futuresï¼Œè€Œä¸æ˜¯forå¾ªç¯
        while futures_dict:
            # ç­‰å¾…ä»»æ„ä¸€ä¸ªä»»åŠ¡å®Œæˆ
            done_futures = []
            try:
                for future in as_completed(futures_dict, timeout=1):
                    done_futures.append(future)
                    break  # åªå¤„ç†ä¸€ä¸ªï¼Œç„¶åæ£€æŸ¥æ—¶é—´å’Œæäº¤æ–°ä»»åŠ¡
            except Exception:
                # timeoutæˆ–å…¶ä»–å¼‚å¸¸ï¼Œç»§ç»­ç­‰å¾…
                pass
            
            if not done_futures:
                continue
            
            future = done_futures[0]
            city, city_id = futures_dict.pop(future)  # ä»å­—å…¸ä¸­ç§»é™¤å·²å®Œæˆçš„
            
            try:
                city_name, result = future.result()
                results[city_name] = result
                completed_in_cycle.append((city, city_id))
            except Exception as e:
                print(f"âœ— Exception for {city_id}: {e}")
                results[city] = None
                completed_in_cycle.append((city, city_id))
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å·¥ä½œçª—å£
            if datetime.now(timezone.utc) >= window_end:
                print(f"\nâ° å·¥ä½œçª—å£å·²åˆ° {work_duration_minutes} åˆ†é’Ÿï¼Œåœæ­¢æäº¤æ–°ä»»åŠ¡...")
                break
            
            # å¦‚æœè¿˜æœ‰å¾…å¤„ç†åŸå¸‚ï¼Œç»§ç»­æäº¤æ–°ä»»åŠ¡
            if city_index < len(pending_cities):
                city, city_id = pending_cities[city_index]
                new_future = executor.submit(
                    scrape_city_wrapper,
                    city,
                    city_id,
                    False,
                    output_root,
                    tracker,
                    first_scrape_date
                )
                futures_dict[new_future] = (city, city_id)
                city_index += 1
        
        # å…³é—­çº¿ç¨‹æ± ï¼Œä¸å†æ¥å—æ–°ä»»åŠ¡ï¼Œä½†ç­‰å¾…å·²æäº¤çš„ä»»åŠ¡å®Œæˆ
        print(f"\nç­‰å¾…æœ¬è½®å‰©ä½™ä»»åŠ¡å®Œæˆ...")
        executor.shutdown(wait=True)
        
        # æ”¶é›†å‰©ä½™æœªå¤„ç†çš„ç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        for future, (city, city_id) in list(futures_dict.items()):
            if (city, city_id) not in completed_in_cycle:
                try:
                    city_name, result = future.result(timeout=0)
                    results[city_name] = result
                    completed_in_cycle.append((city, city_id))
                except Exception as e:
                    print(f"âœ— Exception for {city_id}: {e}")
                    results[city] = None
                    completed_in_cycle.append((city, city_id))
        
        # ä»å¾…å¤„ç†é˜Ÿåˆ—ä¸­ç§»é™¤å·²å®Œæˆçš„åŸå¸‚
        pending_cities = [(c, cid) for c, cid in pending_cities if (c, cid) not in completed_in_cycle]
        
        cycle_end = datetime.now(timezone.utc)
        cycle_duration = (cycle_end - cycle_start).total_seconds() / 60
        
        print(f"\n{'='*60}")
        print(f"ã€ç¬¬ {cycle_num} è½®å·¥ä½œå‘¨æœŸç»“æŸã€‘")
        print(f"å®é™…è¿è¡Œ: {cycle_duration:.1f} åˆ†é’Ÿ")
        print(f"æœ¬è½®å®Œæˆ: {len(completed_in_cycle)} ä¸ªåŸå¸‚")
        print(f"æ€»è¿›åº¦: {tracker.completed}/{total} ({tracker.completed/total*100:.1f}%)")
        print(f"{'='*60}\n")
        
        # å¦‚æœè¿˜æœ‰å¾…å¤„ç†åŸå¸‚ï¼Œä¼‘æ¯åç»§ç»­
        if pending_cities:
            print(f"ğŸ’¤ ä¼‘æ¯ {rest_duration_minutes} åˆ†é’Ÿåç»§ç»­ä¸‹ä¸€è½®...")
            print(f"ä¸‹è½®é¢„è®¡å¼€å§‹æ—¶é—´: {(datetime.now(timezone.utc) + rest_duration).strftime('%Y-%m-%d %H:%M:%S')}\n")
            time.sleep(rest_duration.total_seconds())
            cycle_num += 1
    
    print(f"\n{'='*60}")
    print(f"çˆ¬å–ä»»åŠ¡å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ€»å‘¨æœŸæ•°: {cycle_num}")
    print(f"æˆåŠŸ: {sum(1 for r in results.values() if r)}/{total}")
    print(f"{'='*60}\n")
    
    return results


if __name__ == "__main__":
    import pytz
    
    # å®šä¹‰é…ç½®å‚æ•°
    CSV_FILE = 'nowcast_crawl_list_v3.csv'
    MAX_WORKERS = 3
    BASE_DIR = Path(__file__).parent
    WORK_MINUTES = 20  # å·¥ä½œæ—¶é•¿ï¼š20åˆ†é’Ÿ
    REST_MINUTES = 60  # ä¼‘æ¯æ—¶é•¿ï¼š60åˆ†é’Ÿ
    
    # è®¾ç½®åŒ—äº¬æ—¶åŒº
    beijing_tz = pytz.timezone('Asia/Shanghai')
    
    # ä½¿ç”¨ APScheduler é…ç½® UTC æ—¶åŒºçš„å®šæ—¶ä»»åŠ¡
    scheduler = BackgroundScheduler(timezone='UTC')
    
    # æ¯å¤© UTC æ—¶é—´ 0ã€6ã€12ã€18ç‚¹å„æ‰§è¡Œä¸€æ¬¡
    scheduler.add_job(
        lambda: scrape_all_cities_concurrent(
            base_dir=BASE_DIR, 
            csv_file=CSV_FILE, 
            max_workers=MAX_WORKERS,
            work_duration_minutes=WORK_MINUTES,
            rest_duration_minutes=REST_MINUTES
        ), 
        'cron', 
        hour='18'
    )
    scheduler.start()
    
    print("âœ“ å®šæ—¶çˆ¬è™«å·²å¯åŠ¨ï¼ˆå¾ªç¯å·¥ä½œ/ä¼‘æ¯æ¨¡å¼ï¼‰")
    print(f"âœ“ è¾“å‡ºç›®å½•: {BASE_DIR}")
    print(f"âœ“ CSV æ–‡ä»¶: {CSV_FILE}")
    print(f"âœ“ å·¥ä½œæ¨¡å¼: {WORK_MINUTES}åˆ†é’Ÿå·¥ä½œ / {REST_MINUTES}åˆ†é’Ÿä¼‘æ¯")
    print(f"âœ“ å°†åœ¨æ¯å¤© UTC æ—¶é—´ 00:00, 06:00, 12:00, 18:00 æ‰§è¡Œçˆ¬å–ä»»åŠ¡")
    print(f"âœ“ å¹¶å‘çº¿ç¨‹æ•°: {MAX_WORKERS}") 
    print(f"âœ“ å½“å‰åŒ—äº¬æ—¶é—´: {datetime.now(beijing_tz).strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"âœ“ å½“å‰ UTC æ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')}")
    print("âœ“ æŒ‰ Ctrl+C åœæ­¢ç¨‹åº\n")
    
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼ˆå¯é€‰ï¼‰
    scrape_all_cities_concurrent(
        base_dir=BASE_DIR, 
        csv_file=CSV_FILE, 
        max_workers=MAX_WORKERS,
        work_duration_minutes=WORK_MINUTES,
        rest_duration_minutes=REST_MINUTES
    )
    
    # æŒç»­è¿è¡Œè°ƒåº¦å™¨
    try:
        while True:
            time.sleep(60)
    except KeyboardInterrupt:
        print("\n\nç¨‹åºå·²åœæ­¢")
        scheduler.shutdown()
